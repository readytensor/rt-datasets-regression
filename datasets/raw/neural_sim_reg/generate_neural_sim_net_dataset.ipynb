{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb68ef28",
   "metadata": {},
   "source": [
    "# NeuralSimReg Dataset\n",
    "### A Synthetic Regression Dataset Simulating a Neural Network-Driven Process with Missing Values\n",
    "\n",
    "\n",
    "This synthetic dataset represents a regression problem, simulating a scenario where data is generated by an unknown real-world process, represented here by a neural network. The dataset contains 10,000 samples. Each sample in the dataset has a unique identifier, a set of 16 features, and a continuous target value.\n",
    "\n",
    "The features, originally 16-dimensional data points, are generated from a standard normal distribution (mean = 0 and standard deviation = 1). These features can be conceptualized as random inputs fed into a neural network, which simulates our unknown data generation process, producing a target output.\n",
    "\n",
    "The neural network modeling this process comprises three layers: an input layer of 16 neurons, a first hidden layer of 8 neurons with a ReLU activation, and a second hidden layer of 4 neurons, also with a ReLU activation. The output layer has a single neuron without any activation function, generating a continuous target value for each input sample.\n",
    "\n",
    "Post this neural network generation, as an additional artifact to mirror real-world complications, approximately 2% of the feature values have been made missing, represented as NaN values. This simulates scenarios where datasets might have gaps or missing entries, not due to the data generation process but as a result of data collection, storage, or other external factors.\n",
    "\n",
    "The target value for each sample is the output from this neural network given the original 16-dimensional input features, reflecting the true value produced by our simulated data generation process. The central task is a regression problem, aiming to predict this true target value based on the features, even with their missing entries.\n",
    "\n",
    "The unique challenge posed by this dataset arises from the intricacies introduced by the neural network. With the weights and biases of this neural network initialized randomly from a normal distribution (mean = 0 and standard deviation = 1), the relationship between the input features and the target value is intricate and potentially highly non-linear.\n",
    "\n",
    "Each sample's identifier is a sequential integer, beginning from 1, serving to uniquely identify every sample in the dataset.\n",
    "\n",
    "In essence, this dataset delineates a regression challenge where the objective is to predict the output of an intricate, simulated data generation process based on given input features. The non-linear transformations executed by the neural network, combined with high-dimensionality and introduced missing values in the input, amplify the challenge for regression algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0d3ce246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "12ac8219",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"neural_sim_reg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2ef5611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./../../processed/{dataset_name}/'\n",
    "outp_fname = os.path.join(output_dir, f'{dataset_name}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f449542",
   "metadata": {},
   "source": [
    "# Generation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0d62e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seeds(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Set seeds for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to set for numpy's random operations.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "db4b06a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit (ReLU) activation function.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Input array.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Output after applying the ReLU function element-wise.\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0367b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights_and_biases(\n",
    "        input_size: int, hidden1_size: int, \n",
    "        hidden2_size: int, output_size: int = 1\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for the neural network.\n",
    "    \n",
    "    Args:\n",
    "        input_size (int): Size of the input layer.\n",
    "        hidden1_size (int): Size of the first hidden layer.\n",
    "        hidden2_size (int): Size of the second hidden layer.\n",
    "        output_size (int, optional): Size of the output layer. Defaults to 1.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        W1, b1, W2, b2, W3, b3: Weights and biases for the layers of the neural network.\n",
    "    \"\"\"\n",
    "    W1 = np.random.normal(0, 1, (input_size, hidden1_size))\n",
    "    b1 = np.random.normal(0, 1, (hidden1_size))\n",
    "\n",
    "    W2 = np.random.normal(0, 1, (hidden1_size, hidden2_size))\n",
    "    b2 = np.random.normal(0, 1, (hidden2_size))\n",
    "\n",
    "    W3 = np.random.normal(0, 1, (hidden2_size, output_size))\n",
    "    b3 = np.random.normal(0, 1, (output_size))\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0738d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(\n",
    "        X: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, \n",
    "        b2: np.ndarray, W3: np.ndarray, b3: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Neural network function.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Input data.\n",
    "        W1, b1, W2, b2, W3, b3: Weights and biases for the layers of the\n",
    "                                neural network.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Output from the neural network.\n",
    "    \"\"\"\n",
    "    # First hidden layer\n",
    "    Z1 = X.dot(W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    \n",
    "    # Second hidden layer\n",
    "    Z2 = A1.dot(W2) + b2\n",
    "    A2 = relu(Z2)\n",
    "    \n",
    "    # Output layer\n",
    "    Z3 = A2.dot(W3) + b3\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2877c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(\n",
    "        input_size: int, hidden1_size: int, hidden2_size: int,\n",
    "        num_samples: int = 10000\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic data using a randomly initialized neural network.\n",
    "    \n",
    "    Args:\n",
    "        input_size (int): Size of the input layer.\n",
    "        hidden1_size (int): Size of the first hidden layer.\n",
    "        hidden2_size (int): Size of the second hidden layer.\n",
    "        num_samples (int, optional): Number of data samples to generate.\n",
    "                                     Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the synthetic data.\n",
    "    \"\"\"\n",
    "    # Initialize weights and biases\n",
    "    W1, b1, W2, b2, W3, b3 = initialize_weights_and_biases(\n",
    "        input_size, hidden1_size, hidden2_size\n",
    "    )\n",
    "\n",
    "    # Generate random input data\n",
    "    X = np.random.randn(num_samples, input_size)\n",
    "\n",
    "    # Get the output of the neural network\n",
    "    y = neural_network(X, W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "    # Create a pandas dataframe\n",
    "    df = pd.DataFrame(\n",
    "        X, columns=[f'feature_{i}' for i in range(1, input_size+1)]\n",
    "    )\n",
    "    df['target'] = y\n",
    "    df.insert(0, 'sample_id', np.arange(1, num_samples + 1))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b6f73257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_missing_values(df: pd.DataFrame, frac_missing: float = 0.02) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Introduce missing values in the feature columns of the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Original dataframe.\n",
    "        frac_missing (float): Percentage of values to be replaced with NaN.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with missing values.\n",
    "    \"\"\"\n",
    "    num_rows = len(df)\n",
    "    \n",
    "    feature_cols = df.columns.difference(['sample_id', \"target\"])\n",
    "    num_missing = int(frac_missing * num_rows)\n",
    "    for col in feature_cols: \n",
    "        missing_indices = np.random.choice(num_rows, num_missing, replace=False)\n",
    "        df.loc[missing_indices, col] = np.nan\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b15fe",
   "metadata": {},
   "source": [
    "# Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "516a5699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sample_id  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0          1   1.530773  -0.880927   0.147362  -0.521976  -0.577191   \n",
      "1          2   0.439518   0.219240  -0.763957  -0.612097   0.370604   \n",
      "2          3  -0.227886  -0.859937   1.987863  -1.604954   0.784807   \n",
      "3          4   0.064003   1.426716   1.019955  -1.007368   0.928673   \n",
      "4          5  -1.445058  -0.029162   0.299268  -0.444360  -0.990869   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
      "0   0.034991  -0.845358   1.742985   0.195153   -0.950402    0.611827   \n",
      "1   0.283830   0.082267   0.570342   1.197955   -0.201476    0.156258   \n",
      "2   0.241413  -0.014350  -0.798338  -0.682943    0.983022    0.108191   \n",
      "3  -0.693904   0.808132  -2.569986        NaN    0.365941   -0.613898   \n",
      "4  -0.143019   0.999241   0.967716   0.557419    0.667822   -2.228937   \n",
      "\n",
      "   feature_12  feature_13  feature_14  feature_15  feature_16     target  \n",
      "0    0.858426    1.778379    1.570481    0.556079    0.567018 -12.226615  \n",
      "1   -1.602080   -0.192584    1.911318   -0.043332    0.193384  -7.873196  \n",
      "2   -0.028221         NaN    0.995822   -1.685493    1.244342  -0.395675  \n",
      "3    0.588773   -1.369248   -0.882519    0.683309    1.476080  -0.502695  \n",
      "4   -1.002577   -0.434471    1.271451   -0.264690   -0.342037  -0.510236  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 18)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_random_seeds(66)\n",
    "# Generate the synthetic dataset with custom input size and hidden layer sizes\n",
    "orig_data = generate_synthetic_data(num_samples=10000, input_size=16, hidden1_size=8, hidden2_size=4)\n",
    "# Introduce missing values\n",
    "data = introduce_missing_values(orig_data, frac_missing=0.02)\n",
    "print(data.head())\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f24e6",
   "metadata": {},
   "source": [
    "# Save Main Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "653a75bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.to_csv(outp_fname, index=False, float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94315897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
